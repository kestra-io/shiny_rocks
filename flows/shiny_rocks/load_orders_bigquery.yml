id: load_orders_bigquery
namespace: shiny_rocks
description: |
  When data are generated upstream, this flow ingest the `orders` data into Google Cloud Storage and BigQuery.

labels:
  tag: load

inputs:
  - name: orders_data
    type: URI

  - name: order_date
    type: DATE

tasks:

  - id: extract
    type: io.kestra.plugin.gcp.gcs.Upload
    serviceAccount: '{{ secret("GCP_CREDS") }}'
    from: "{{ inputs.orders_data }}"
    to: gs://shiny_rocks/app_log/orders/{{ inputs.order_date }}/orders.csv

  - id: load
    type: io.kestra.plugin.gcp.bigquery.LoadFromGcs
    from: 
      - "{{ outputs.extract.uri }}"
    projectId: "kestra-dev"
    destinationTable: "shiny_rocks.orders"
    serviceAccount: '{{ secret("GCP_CREDS") }}'
    format: CSV
    autodetect: true
    csvOptions:
      fieldDelimiter: ","
    timePartitioningField: "order_date"

triggers:

  - id: get_data
    type: io.kestra.core.models.triggers.types.Flow
    inputs:
      orders_data: "{{ outputs.python.outputFiles['orders.csv'] }}"
      order_date: "{{ outputs.run_date.value }}"
    conditions:
      - type: io.kestra.core.models.conditions.types.ExecutionFlowCondition
        namespace: shiny_rocks
        flowId: produce_data
      - type: io.kestra.core.models.conditions.types.ExecutionStatusCondition
        in:
          - SUCCESS