id: load_services_bigquery
namespace: shiny_rocks
description: |
  When data are generated upstream, this flow ingest the `services` data into Google Cloud Storage and BigQuery.

labels:
  tag: load

inputs:
  - name: services_data
    type: URI

  - name: run_date
    type: DATE

tasks:

  - id: extract
    type: io.kestra.plugin.gcp.gcs.Upload
    from: "{{ inputs.services_data }}"
    to: gs://shiny_rocks/app_log/services/{{ inputs.run_date }}/services.csv

  - id: load
    type: io.kestra.plugin.gcp.bigquery.LoadFromGcs
    from: 
      - "{{ outputs.extract.uri }}"
    projectId: "kestra-dev"
    destinationTable: "shiny_rocks.services"
    format: CSV
    autodetect: true
    csvOptions:
      fieldDelimiter: ","
    timePartitioningField: "run_date"

triggers:

  - id: get_data
    type: io.kestra.core.models.triggers.types.Flow
    inputs:
      services_data: "{{ outputs.python.outputFiles['services.csv'] }}"
      run_date: "{{ outputs.run_date.value }}"
    conditions:
      - type: io.kestra.core.models.conditions.types.ExecutionFlowCondition
        namespace: shiny_rocks
        flowId: produce_data
      - type: io.kestra.core.models.conditions.types.ExecutionStatusCondition
        in:
          - SUCCESS